# -*- coding:utf-8 -*-
"""
VAD (Voice Activity Detection) è¯­éŸ³æ´»åŠ¨æ£€æµ‹æ¨¡å—
ä½¿ç”¨ Silero VAD å®ç°é«˜ç²¾åº¦è¯­éŸ³æ£€æµ‹
"""

import torch
import numpy as np
from silero_vad import load_silero_vad, get_speech_timestamps


class VADDetector:
    """åŸºäºSilero VADçš„è¯­éŸ³æ´»åŠ¨æ£€æµ‹å™¨"""

    def __init__(self,
                 sample_rate=16000,
                 threshold=0.5,
                 min_speech_duration_ms=250,
                 min_silence_duration_ms=500,
                 window_size_samples=512):
        """
        åˆå§‹åŒ–VADæ£€æµ‹å™¨

        å‚æ•°è¯´æ˜ï¼š
        :param sample_rate: é‡‡æ ·ç‡ï¼ˆHzï¼‰ï¼Œå¿…é¡»æ˜¯16000
        :param threshold: è¯­éŸ³åˆ¤å®šé˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œè¶Šå¤§è¶Šä¸¥æ ¼
                         - 0.3: å®½æ¾ï¼Œå®¹æ˜“è§¦å‘ï¼ˆå™ªéŸ³ç¯å¢ƒç”¨ï¼‰
                         - 0.5: æ ‡å‡†ï¼Œå¹³è¡¡
                         - 0.7: ä¸¥æ ¼ï¼Œåªæ£€æµ‹æ˜ç¡®è¯­éŸ³
        :param min_speech_duration_ms: æœ€å°è¯­éŸ³æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
                                       ä½äºæ­¤æ—¶é•¿çš„è¯­éŸ³è¢«å¿½ç•¥ï¼ˆé˜²è¯¯è§¦å‘ï¼‰
        :param min_silence_duration_ms: æœ€å°é™éŸ³æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
                                        è¿ç»­é™éŸ³è¶…è¿‡æ­¤æ—¶é•¿æ‰è®¤ä¸ºè¯´è¯ç»“æŸ
        :param window_size_samples: å¤„ç†çª—å£å¤§å°ï¼ˆæ ·æœ¬æ•°ï¼‰
                                   512 = 32ms (æ¨è)
        """
        self.sample_rate = sample_rate
        self.threshold = threshold
        self.min_speech_duration_ms = min_speech_duration_ms
        self.min_silence_duration_ms = min_silence_duration_ms
        self.window_size_samples = window_size_samples

        # åŠ è½½Silero VADæ¨¡å‹
        print("ğŸ”„ æ­£åœ¨åŠ è½½VADæ¨¡å‹...")
        self.model = load_silero_vad()
        print("âœ… VADæ¨¡å‹åŠ è½½å®Œæˆï¼")

        # çŠ¶æ€å˜é‡
        self.reset()

    def reset(self):
        """é‡ç½®æ£€æµ‹å™¨çŠ¶æ€"""
        self.is_speaking = False  # å½“å‰æ˜¯å¦åœ¨è¯´è¯
        self.speech_frames = 0  # è¿ç»­è¯­éŸ³å¸§è®¡æ•°
        self.silence_frames = 0  # è¿ç»­é™éŸ³å¸§è®¡æ•°
        self.total_frames = 0  # æ€»å¸§æ•°

        # è®¡ç®—å¸§æ•°é˜ˆå€¼
        ms_per_frame = (self.window_size_samples / self.sample_rate) * 1000
        self.min_speech_frames = int(self.min_speech_duration_ms / ms_per_frame)
        self.min_silence_frames = int(self.min_silence_duration_ms / ms_per_frame)

    def process_frame(self, audio_frame):
        """
        å¤„ç†å•ä¸ªéŸ³é¢‘å¸§

        :param audio_frame: éŸ³é¢‘æ•°æ®ï¼ˆbytesæˆ–numpy arrayï¼‰
        :return: (is_speech, should_stop)
                 is_speech: å½“å‰å¸§æ˜¯å¦ä¸ºè¯­éŸ³
                 should_stop: æ˜¯å¦åº”è¯¥åœæ­¢å½•éŸ³
        """
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(audio_frame, bytes):
            audio_np = np.frombuffer(audio_frame, dtype=np.int16)
        else:
            audio_np = audio_frame

        # å½’ä¸€åŒ–åˆ° [-1, 1]
        audio_float = audio_np.astype(np.float32) / 32768.0

        # ç¡®ä¿é•¿åº¦æ­£ç¡®
        if len(audio_float) != self.window_size_samples:
            # å¡«å……æˆ–æˆªæ–­
            if len(audio_float) < self.window_size_samples:
                audio_float = np.pad(audio_float,
                                     (0, self.window_size_samples - len(audio_float)))
            else:
                audio_float = audio_float[:self.window_size_samples]

        # è½¬æ¢ä¸ºTensor
        audio_tensor = torch.from_numpy(audio_float)

        # VADæ£€æµ‹
        speech_prob = self.model(audio_tensor, self.sample_rate).item()

        # åˆ¤æ–­æ˜¯å¦ä¸ºè¯­éŸ³
        is_speech = speech_prob > self.threshold

        # æ›´æ–°çŠ¶æ€
        self.total_frames += 1

        if is_speech:
            self.speech_frames += 1
            self.silence_frames = 0

            # æ£€æµ‹åˆ°è¶³å¤Ÿé•¿çš„è¯­éŸ³ï¼Œæ ‡è®°ä¸º"æ­£åœ¨è¯´è¯"
            if self.speech_frames >= self.min_speech_frames:
                self.is_speaking = True
        else:
            self.silence_frames += 1
            if self.is_speaking:
                self.speech_frames = 0

        # åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢
        should_stop = (
                self.is_speaking and
                self.silence_frames >= self.min_silence_frames
        )

        return is_speech, should_stop

    def process_audio_batch(self, audio_data):
        """
        å¤„ç†æ•´æ®µéŸ³é¢‘ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰
        ç”¨äºåˆ†æå·²å½•åˆ¶çš„éŸ³é¢‘

        :param audio_data: å®Œæ•´éŸ³é¢‘æ•°æ®ï¼ˆbytesæˆ–numpy arrayï¼‰
        :return: åŒ…å«è¯­éŸ³ç‰‡æ®µçš„æ—¶é—´æˆ³åˆ—è¡¨ [(start_ms, end_ms), ...]
        """
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(audio_data, bytes):
            audio_np = np.frombuffer(audio_data, dtype=np.int16)
        else:
            audio_np = audio_data

        # å½’ä¸€åŒ–
        audio_float = audio_np.astype(np.float32) / 32768.0

        # è½¬æ¢ä¸ºTensor
        audio_tensor = torch.from_numpy(audio_float)

        # è·å–è¯­éŸ³æ—¶é—´æˆ³
        timestamps = get_speech_timestamps(
            audio_tensor,
            self.model,
            sampling_rate=self.sample_rate,
            threshold=self.threshold,
            min_speech_duration_ms=self.min_speech_duration_ms,
            min_silence_duration_ms=self.min_silence_duration_ms
        )

        # è½¬æ¢ä¸ºæ¯«ç§’
        result = []
        for ts in timestamps:
            start_ms = int(ts['start'] / self.sample_rate * 1000)
            end_ms = int(ts['end'] / self.sample_rate * 1000)
            result.append((start_ms, end_ms))

        return result


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¤ VADæ£€æµ‹å™¨æµ‹è¯•")
    print("=" * 60)

    # åˆ›å»ºVADæ£€æµ‹å™¨
    vad = VADDetector(
        threshold=0.5,
        min_speech_duration_ms=250,
        min_silence_duration_ms=500
    )

    print("\nâœ… VADæ£€æµ‹å™¨åˆ›å»ºæˆåŠŸï¼")
    print(f"ğŸ“Š é…ç½®å‚æ•°ï¼š")
    print(f"   - é‡‡æ ·ç‡: {vad.sample_rate} Hz")
    print(f"   - é˜ˆå€¼: {vad.threshold}")
    print(f"   - æœ€å°è¯­éŸ³æ—¶é•¿: {vad.min_speech_duration_ms} ms")
    print(f"   - æœ€å°é™éŸ³æ—¶é•¿: {vad.min_silence_duration_ms} ms")
    print(f"   - çª—å£å¤§å°: {vad.window_size_samples} æ ·æœ¬")

    # æµ‹è¯•å•å¸§å¤„ç†
    print("\nğŸ§ª æµ‹è¯•å•å¸§å¤„ç†...")
    test_frame = np.zeros(512, dtype=np.int16)  # é™éŸ³å¸§
    is_speech, should_stop = vad.process_frame(test_frame)
    print(f"   é™éŸ³å¸§æµ‹è¯•: is_speech={is_speech}, should_stop={should_stop}")

    print("\nâœ… VADæ¨¡å—æµ‹è¯•å®Œæˆï¼")