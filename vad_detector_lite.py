# -*- coding:utf-8 -*-
"""
è½»é‡çº§ VAD (Voice Activity Detection) æ¨¡å—
ä½¿ç”¨ WebRTC VAD - ä¸“ä¸ºæ ‘è“æ´¾ Zero 2W ä¼˜åŒ–
"""

import webrtcvad
import collections
import time


class VADDetectorLite:
    """åŸºäº WebRTC VAD çš„è½»é‡çº§è¯­éŸ³æ´»åŠ¨æ£€æµ‹å™¨"""
    
    def __init__(self, 
                 sample_rate=16000,
                 aggressiveness=2,
                 frame_duration_ms=30,
                 padding_duration_ms=300,
                 min_silence_duration_ms=1500):
        """
        åˆå§‹åŒ–è½»é‡çº§ VAD æ£€æµ‹å™¨
        
        :param sample_rate: é‡‡æ ·ç‡ï¼ŒWebRTC VAD æ”¯æŒ 8000/16000/32000/48000
        :param aggressiveness: æ•æ„Ÿåº¦ 0-3
                              0: è´¨é‡ä¼˜å…ˆï¼ˆä¸æ•æ„Ÿï¼‰
                              1: å¹³è¡¡
                              2: æ£€æµ‹ä¼˜å…ˆï¼ˆè¾ƒæ•æ„Ÿï¼‰- æ¨è
                              3: æœ€æ•æ„Ÿ
        :param frame_duration_ms: å¸§æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰ï¼Œåªèƒ½æ˜¯ 10/20/30
        :param padding_duration_ms: è¯­éŸ³å‰åç¼“å†²æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
        :param min_silence_duration_ms: æœ€å°é™éŸ³æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰ï¼Œè¶…è¿‡æ­¤æ—¶é•¿è®¤ä¸ºè¯´è¯ç»“æŸ
        """
        # å‚æ•°éªŒè¯
        if sample_rate not in [8000, 16000, 32000, 48000]:
            raise ValueError("é‡‡æ ·ç‡å¿…é¡»æ˜¯ 8000, 16000, 32000 æˆ– 48000")
        
        if frame_duration_ms not in [10, 20, 30]:
            raise ValueError("å¸§æ—¶é•¿å¿…é¡»æ˜¯ 10, 20 æˆ– 30 æ¯«ç§’")
        
        if aggressiveness not in [0, 1, 2, 3]:
            raise ValueError("æ•æ„Ÿåº¦å¿…é¡»æ˜¯ 0-3")
        
        self.sample_rate = sample_rate
        self.aggressiveness = aggressiveness
        self.frame_duration_ms = frame_duration_ms
        self.padding_duration_ms = padding_duration_ms
        self.min_silence_duration_ms = min_silence_duration_ms
        
        # åˆ›å»º VAD å®ä¾‹
        self.vad = webrtcvad.Vad(aggressiveness)
        
        # è®¡ç®—å¸§å¤§å°ï¼ˆå­—èŠ‚æ•°ï¼‰
        # å…¬å¼ï¼šé‡‡æ ·ç‡ * å¸§æ—¶é•¿(ç§’) * 2å­—èŠ‚(int16)
        self.frame_size_bytes = int(sample_rate * frame_duration_ms / 1000 * 2)
        
        # è®¡ç®—ç¼“å†²å¸§æ•°
        self.padding_frames = int(padding_duration_ms / frame_duration_ms)
        self.silence_frames = int(min_silence_duration_ms / frame_duration_ms)
        
        # çŠ¶æ€å˜é‡
        self.reset()
        
        print(f"âœ… WebRTC VAD åˆå§‹åŒ–å®Œæˆ")
        print(f"   - é‡‡æ ·ç‡: {sample_rate} Hz")
        print(f"   - æ•æ„Ÿåº¦: {aggressiveness} (0=ä½ 3=é«˜)")
        print(f"   - å¸§å¤§å°: {self.frame_size_bytes} å­—èŠ‚ ({frame_duration_ms}ms)")
    
    def reset(self):
        """é‡ç½®æ£€æµ‹å™¨çŠ¶æ€"""
        self.is_speaking = False
        self.triggered = False
        self.voiced_frames = []  # ç¼“å†²åŒº
        self.ring_buffer = collections.deque(maxlen=self.padding_frames)
        self.silence_counter = 0
        self.total_frames = 0
    
    def process_frame(self, audio_frame):
        """
        å¤„ç†å•ä¸ªéŸ³é¢‘å¸§
        
        :param audio_frame: éŸ³é¢‘æ•°æ®ï¼ˆbytesï¼‰ï¼Œé•¿åº¦å¿…é¡»ç­‰äº frame_size_bytes
        :return: (is_speech, should_stop, buffered_audio)
                 is_speech: å½“å‰å¸§æ˜¯å¦ä¸ºè¯­éŸ³
                 should_stop: æ˜¯å¦åº”è¯¥åœæ­¢å½•éŸ³
                 buffered_audio: ç¼“å†²çš„éŸ³é¢‘æ•°æ®ï¼ˆbytesï¼‰ï¼Œç”¨äºè¿”å›å®Œæ•´è¯­éŸ³
        """
        # éªŒè¯å¸§å¤§å°
        if len(audio_frame) != self.frame_size_bytes:
            # è‡ªåŠ¨è°ƒæ•´ï¼ˆå¡«å……æˆ–æˆªæ–­ï¼‰
            if len(audio_frame) < self.frame_size_bytes:
                audio_frame = audio_frame + b'\x00' * (self.frame_size_bytes - len(audio_frame))
            else:
                audio_frame = audio_frame[:self.frame_size_bytes]
        
        # VAD æ£€æµ‹
        is_speech = self.vad.is_speech(audio_frame, self.sample_rate)
        
        self.total_frames += 1
        should_stop = False
        buffered_audio = None
        
        if not self.triggered:
            # æœªè§¦å‘çŠ¶æ€ï¼šç­‰å¾…æ£€æµ‹åˆ°è¯­éŸ³
            self.ring_buffer.append((audio_frame, is_speech))
            num_voiced = len([f for f, speech in self.ring_buffer if speech])
            
            # å¦‚æœç¼“å†²åŒºä¸­æœ‰è¶³å¤Ÿå¤šçš„è¯­éŸ³å¸§ï¼Œè§¦å‘å½•éŸ³
            if num_voiced > 0.5 * self.ring_buffer.maxlen:
                self.triggered = True
                self.is_speaking = True
                # å°†ç¼“å†²åŒºçš„å†…å®¹åŠ å…¥å½•éŸ³
                self.voiced_frames.extend([f for f, s in self.ring_buffer])
                self.ring_buffer.clear()
                print("ğŸ—£ï¸  æ£€æµ‹åˆ°è¯­éŸ³ï¼Œå¼€å§‹å½•éŸ³...")
        else:
            # å·²è§¦å‘çŠ¶æ€ï¼šå½•éŸ³ä¸­
            self.voiced_frames.append(audio_frame)
            self.ring_buffer.append((audio_frame, is_speech))
            num_unvoiced = len([f for f, speech in self.ring_buffer if not speech])
            
            # å¦‚æœç¼“å†²åŒºä¸­å¤§éƒ¨åˆ†æ˜¯é™éŸ³å¸§ï¼Œå¢åŠ é™éŸ³è®¡æ•°
            if num_unvoiced > 0.9 * self.ring_buffer.maxlen:
                self.silence_counter += 1
            else:
                self.silence_counter = 0
            
            # å¦‚æœé™éŸ³æŒç»­è¶³å¤Ÿé•¿ï¼Œåœæ­¢å½•éŸ³
            if self.silence_counter >= self.silence_frames:
                should_stop = True
                self.is_speaking = False
                # è¿”å›å®Œæ•´çš„å½•éŸ³æ•°æ®
                buffered_audio = b''.join(self.voiced_frames)
        
        return is_speech, should_stop, buffered_audio
    
    def get_frame_size_bytes(self):
        """è·å–å¸§å¤§å°ï¼ˆå­—èŠ‚æ•°ï¼‰"""
        return self.frame_size_bytes


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¤ è½»é‡çº§ VAD æ£€æµ‹å™¨æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»º VAD æ£€æµ‹å™¨
    vad = VADDetectorLite(
        sample_rate=16000,
        aggressiveness=2,
        frame_duration_ms=30,
        padding_duration_ms=300,
        min_silence_duration_ms=800
    )
    
    print(f"\nğŸ“Š å¸§å¤§å°: {vad.get_frame_size_bytes()} å­—èŠ‚")
    print("âœ… VAD æ£€æµ‹å™¨åˆ›å»ºæˆåŠŸï¼")
    
    # æµ‹è¯•å•å¸§å¤„ç†
    print("\nğŸ§ª æµ‹è¯•å•å¸§å¤„ç†...")
    import numpy as np
    test_frame = np.zeros(vad.frame_size_bytes, dtype=np.uint8).tobytes()
    is_speech, should_stop, audio = vad.process_frame(test_frame)
    print(f"   é™éŸ³å¸§æµ‹è¯•: is_speech={is_speech}, should_stop={should_stop}")
    
    print("\nâœ… VAD æ¨¡å—æµ‹è¯•å®Œæˆï¼")

